# -*- coding: utf-8 -*-
"""Resume_and_job_matching_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11BuDsgMrjTUPt6PLiDnItCwB28uActAj
"""

!pip install transformers

import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import classification_report, accuracy_score



nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

df = pd.read_csv(r"/content/drive/MyDrive/Colab Notebooks/job_applicant_dataset.csv")
df.info()

from google.colab import drive
drive.mount('/content/drive')

df.head(5)

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoTokenizer
from collections import Counter
import re
import numpy as np
import pandas as pd


df['Resume'] = df['Resume'].str.replace(r"\bProficient in\b", "", regex=True)
df['Resume'] = df['Resume'].str.replace(r"\s+", " ", regex=True).str.strip()
df['Job Description'] = df['Job Description'].str.replace(r"\s+", " ", regex=True).str.strip()

df['Resume']

def tokenize(text):
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text.lower())  # keep alphanumerics
    return text.split()

df['combined_text'] = "[RESUME] " + df['Resume'] + " [JOB] " + df['Job Description']
tokenized_texts = [tokenize(t) for t in df['combined_text']]

#Building Vocabulary
max_vocab_size = 20000
all_tokens = [tok for txt in tokenized_texts for tok in txt]
vocab = {word: idx+2 for idx, (word, _) in enumerate(
    Counter(all_tokens).most_common(max_vocab_size)
)}
vocab["<PAD>"] = 0
vocab["<UNK>"] = 1

def encode(tokens):
    return torch.tensor([vocab.get(tok, 1) for tok in tokens], dtype=torch.long)

encoded_texts = [encode(t) for t in tokenized_texts]

from sklearn.model_selection import train_test_split
import numpy as np

labels = df['Best Match'].tolist()

#Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    encoded_texts, labels, test_size=0.2, random_state=42
)

X_train = np.array(X_train, dtype=object)
X_test = np.array(X_test, dtype=object)
y_train = np.array(y_train)
y_test = np.array(y_test)

class ResumeDataset(Dataset):
    def __init__(self, texts, labels, max_len=256):
        self.texts = [t[:max_len] for t in texts]  # truncate
        self.labels = torch.tensor(labels, dtype=torch.float32)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.texts[idx], self.labels[idx]

def collate_fn(batch):
    texts, labels = zip(*batch)
    padded = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=0)
    return padded, torch.stack(labels)

#DataLoaders
train_loader = DataLoader(
    ResumeDataset(X_train, y_train),
    batch_size=2,
    collate_fn=collate_fn,
    shuffle=True
)

test_loader = DataLoader(
    ResumeDataset(X_test, y_test),
    batch_size=2,
    collate_fn=collate_fn
)

#LSTM Model
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=3):
        super(LSTMClassifier, self).__init__()
        self.embedding=nn.Embedding(vocab_size, 100, padding_idx=0)
        self.lstm=nn.LSTM(100, 128, batch_first=True, bidirectional=True, num_layers=3)
        self.dropout = nn.Dropout(0.2)

        self.attain = nn.Linear(128*2, 1)

        self.fc = nn.Linear(128*2, 512)
        self.output = nn.Linear(512, 1)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
      emb = self.embedding(x)
      out, _ = self.lstm(emb)

      attn_weights = self.attain(out)
      attn_weights = torch.softmax(attn_weights, dim=1)

      context_vector = torch.sum(attn_weights*out, dim=1)

      out = self.dropout(context_vector)
      out = F.relu(self.fc(out))
      out = self.output(out)

      return self.sigmoid(out).squeeze()

#Training
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMClassifier(len(vocab), embed_dim=50, hidden_dim=64).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
    model.train()
    total_loss = 0
    for batch_x, batch_y in train_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        optimizer.zero_grad()
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}")

#Evaluation
model.eval()
correct, total = 0, 0
with torch.no_grad():
    for batch_x, batch_y in test_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        outputs = model(batch_x)
        preds = (outputs >= 0.5).float()
        correct += (preds == batch_y).sum().item()
        total += batch_y.size(0)

print(f"Test Accuracy: {correct/total:.2f}")